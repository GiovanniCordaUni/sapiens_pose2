# Sapiens Pose Estimation Pipeline (COCO17)

Pipeline per la stima della posa umana da video di esercizi di fisiologia.

## Modelli utilizzati
- **YOLO v8**: Person detection
- **Meta Sapiens 0.3B**: Pose estimation (COCO 17 keypoints nativi)
- **Meta Sapiens 0.6B**: Pose estimation (COCO 17 keypoints nativi)
- **Meta Sapiens 1B**: Pose estimation (COCO 17 keypoints nativi)

## Features
- Elaborazione singolo video o dataset completo
- Output JSONL con keypoints + video overlay opzionale
- Estrazione automatica del soggetto dal path
- Configurazione via YAML
- Struttura modulare

## Installazione

```bash
# Crea ambiente conda
conda create -n sapiens-pose python=3.11
conda activate sapiens-pose

# Installa dipendenze
pip install -r requirements.txt
```

## Utilizzo

### Singolo video
```bash
python scripts/run_pose_estimation.py --video data/input/videos/soggetto001/nome_video.mp4
```

Il soggetto viene estratto automaticamente dal path. Output:
```
data/output/soggetto001/nome_video.jsonl
data/output/soggetto001/nome_video.mp4
```

### Dataset completo
```bash
python scripts/run_pose_estimation.py --dataset data/input/videos
```

Elabora tutti i video in `data/input/videos/soggettoNNN/*.mp4`

### Opzioni utili
```bash
--no-video          # Solo JSONL, salta video overlay
--stride N          # Elabora 1 frame ogni N
--skip-existing     # Salta video già elaborati
--device cuda       # Forza GPU (default: auto)
```

## Struttura del progetto

```
sapiens_pose2/
├── config/
│   ├── default.yaml          # Configurazione generale
│   └── keypoints.yaml        # Definizione COCO 17 keypoints
├── scripts/
│   └── run_pose_estimation.py  # Script CLI principale
├── src/
│   ├── detection/
│   │   └── person_detector.py  # Utilities bounding box
│   ├── io/
│   │   ├── jsonl_writer.py     # Output JSONL
│   │   ├── video_reader.py     # Lettura video
│   │   └── video_writer.py     # Scrittura video
│   ├── loaders/
│   │   ├── sapiens_loader.py   # Wrapper Sapiens
│   │   └── yolo_loader.py      # Wrapper YOLO
│   ├── pipeline/
│   │   └── pipeline_video.py   # Orchestrazione pipeline
│   ├── pose/
│   │   ├── decode.py           # Decodifica heatmaps
│   │   └── preprocess.py       # Preprocessing immagini
│   └── viz/
│       └── draw.py             # Disegno skeleton
├── data/
│   ├── input/videos/
│   │   └── soggettoNNN/        # Video input per soggetto
│   └── output/
│       └── soggettoNNN/        # Output per soggetto
│           ├── video.jsonl
│           └── video_overlay.mp4
├── weights/
│   └── yolo/
│       └── yolov8n.pt          # Pesi YOLO
├── sapiens_host/
│   └── pose/checkpoints/
│       └── sapiens_0.3b/
│       |    └── sapiens_0.3b_coco_best_coco_AP_796_torchscript.
pt2
├── requirements.txt
└── README.md
```

## Formato output JSONL

Ogni riga contiene un frame:

```json
{
  "frame": 0,
  "time_sec": 0.0,
  "person_box_xyxy": [100, 50, 300, 400],
  "person_box_conf": 0.95,
  "status": "ok",
  "keypoints17": [
    [150.0, 80.0, 0.92],
    [155.0, 75.0, 0.88],
    ...
  ]
}
```

### COCO 17 Keypoints
| Indice | Nome |
|--------|------|
| 0 | nose |
| 1 | left_eye |
| 2 | right_eye |
| 3 | left_ear |
| 4 | right_ear |
| 5 | left_shoulder |
| 6 | right_shoulder |
| 7 | left_elbow |
| 8 | right_elbow |
| 9 | left_wrist |
| 10 | right_wrist |
| 11 | left_hip |
| 12 | right_hip |
| 13 | left_knee |
| 14 | right_knee |
| 15 | left_ankle |
| 16 | right_ankle |

## Note

- I pesi dei modelli NON sono inclusi nel repository
- Il checkpoint Sapiens produce direttamente 17 keypoints COCO (nessun mapping necessario)
- Testato con Python 3.11, PyTorch 2.x, CUDA 12.x